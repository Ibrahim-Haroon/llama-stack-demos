name: Llama stack builds
on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Llama stack release (e.g. 0.1.8, 0.1.9)'
        required: true
        type: string
        default: '0.1.9'
      external_provider:
        description: 'Enable external providers directory'
        required: false
        type: boolean
        default: true

jobs:
  llama-stack-build:
    runs-on: ubuntu-latest
    environment: ci
    steps:
      - name: Checkout release tag
        uses: actions/checkout@v4
        with:
           ref: v${{ inputs.version }}
           repository: meta-llama/llama-stack.git

      - name: python deps
        run: |
           sudo apt-get update
           sudo apt-get install -y python3-pip

      - name: login to quay.io
        uses: docker/login-action@v3
        with:
          registry: quay.io
          username: ${{ secrets.QUAY_USERNAME }}
          password: ${{ secrets.QUAY_PASSWORD }}

      - name: Install repo packages
        run: |
          pip install -U .
          cd ..

      - name: clone the llama stack demo repo
        uses: actions/checkout@v4
        with:
          repository: opendatahub-io/llama-stack-demos.git
          ref: main

      - name: Modify build.yaml if external_provider is false
        if: ${{ inputs.external_provider == false }}
        run: |
          cd distribution
          # Comment out the external_providers_dir line
          sed -i 's/^external_providers_dir: /#external_providers_dir: /' build.yaml
          # Replace remote::lmeval with inline::meta-reference in the eval section
          sed -i '/eval:/,/datasetio:/ s/remote::lmeval/inline::meta-reference/' build.yaml
          # Comment out the remote::openai line
          sed -i '/inference:/,/vector_io:/ s/    - remote::openai/    #- remote::openai/' build.yaml

      - name: run llama stack build
        run: |
          cd distribution
          export CONTAINER_BINARY=podman
          llama stack build --config build.yaml

      - name: tag the image (with external)
        if: ${{ inputs.external_provider == true }}
        run: |
          podman tag localhost/llama-stack-demos:${{inputs.version}} quay.io/redhat-et/llama:vllm-${{inputs.version}}-external

      - name: push the image (with external)
        if: ${{ inputs.external_provider == true }}
        run: |
          podman push quay.io/redhat-et/llama:vllm-${{inputs.version}}-external
          
      - name: tag the image (without external)
        if: ${{ inputs.external_provider == false }}
        run: |
          podman tag localhost/llama-stack-demos:${{inputs.version}} quay.io/redhat-et/llama:vllm-${{inputs.version}}

      - name: push the image (without external)
        if: ${{ inputs.external_provider == false }}
        run: |
          podman push quay.io/redhat-et/llama:vllm-${{inputs.version}}
