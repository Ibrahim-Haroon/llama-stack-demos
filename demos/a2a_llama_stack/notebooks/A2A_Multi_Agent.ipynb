{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55ab371",
   "metadata": {},
   "source": [
    "# A2A Multi Agent - Quick-Start Notebook\n",
    "\n",
    "Welcome to the A2A Multi-Agent Quick-Start notebook! This guide will show you how to set up and interact with a multi-agent system using the Agent-to-Agent (A2A) protocol within the Llama Stack environment.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook guides you through the following key steps to set up and interact with a multi-agent system:\n",
    "\n",
    "1.  **Setting up the Environment**: Preparing your Python environment by installing necessary libraries and configuring asynchronous operations.\n",
    "\n",
    "2.  **Agent Management**: Understanding how to connect to and manage the different agents within the system.\n",
    "\n",
    "3.  **Defining Agent Task Flow**: Exploring the multi-phase process (planning, execution, and composition) by which agents collaboratively solve a query.\n",
    "\n",
    "4.  **Launching Agent Servers**: Starting the Agent-to-Agent (A2A) orchestrator and skill agent servers.\n",
    "\n",
    "5.  **Interacting with Agents**: Sending questions to the agent team and observing their orchestrated responses.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure that you have:\n",
    "\n",
    "* `python_requires >= 3.13`.\n",
    "\n",
    "* Completed the initial setup as outlined in the [Setup Guide](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb) notebook.\n",
    "\n",
    "## Environment Variables\n",
    "\n",
    "This notebook is designed to be flexible, allowing you to connect to either a local or a remote Llama Stack instance, and to specify the inference models used by the agents. You can configure these aspects using the following environment variables:\n",
    "\n",
    "* `REMOTE_BASE_URL`: Set this variable if you intend to connect to a **remote Llama Stack instance**. If this variable is not set, the notebook will default to running with a local instance.\n",
    "\n",
    "* `INFERENCE_MODEL_ID`: Define this variable to specify the default Large Language Model (LLM) that agents should use for inference. We recommend using `llama3.1:8b-instruct-fp16` for optimal performance.\n",
    "\n",
    "**Note on Agent-Specific Models:**\n",
    "If you require different inference models for individual agents, you can achieve this by directly opening and modifying the `__main__.py` file within each respective agent's folder (e.g. `demos/a2a_llama_stack/agents/a2a_custom_tools/__main__.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853d0ba",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Notebook Environment\n",
    "\n",
    "We'll start by setting up the necessary environment and installing the required packages to enable A2A communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a1075",
   "metadata": {},
   "source": [
    "We'll install the official [A2A sample implementation by Google](https://github.com/google/A2A/tree/main/samples/python), the Llama Stack client, and other essential packages for asynchronous operations in Jupyter. Run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5bce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/google/A2A.git#subdirectory=samples/python\n",
      "  Cloning https://github.com/google/A2A.git to /private/var/folders/p4/635191ns4599kwjkqt12kwd80000gn/T/pip-req-build-11e356th\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google/A2A.git /private/var/folders/p4/635191ns4599kwjkqt12kwd80000gn/T/pip-req-build-11e356th\n",
      "  Resolved https://github.com/google/A2A.git to commit 081fa20bdfede24922c49e8e56fcdfbee0db0c28\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: a2a-sdk>=0.2.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (0.2.1)\n",
      "Requirement already satisfied: httpx-sse>=0.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (0.28.1)\n",
      "Requirement already satisfied: jwcrypto>=1.5.6 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (1.5.6)\n",
      "Requirement already satisfied: pydantic>=2.10.6 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (2.11.4)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (2.10.1)\n",
      "Requirement already satisfied: sse-starlette>=2.2.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (2.3.5)\n",
      "Requirement already satisfied: starlette>=0.46.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (0.46.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.34.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-samples==0.1.0) (0.34.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.33.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-sdk>=0.2.1->a2a-samples==0.1.0) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.33.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from a2a-sdk>=0.2.1->a2a-samples==0.1.0) (1.33.1)\n",
      "Requirement already satisfied: anyio in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from httpx>=0.28.1->a2a-samples==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from httpx>=0.28.1->a2a-samples==0.1.0) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from httpx>=0.28.1->a2a-samples==0.1.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from httpx>=0.28.1->a2a-samples==0.1.0) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.28.1->a2a-samples==0.1.0) (0.16.0)\n",
      "Requirement already satisfied: cryptography>=3.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from jwcrypto>=1.5.6->a2a-samples==0.1.0) (45.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pydantic>=2.10.6->a2a-samples==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pydantic>=2.10.6->a2a-samples==0.1.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pydantic>=2.10.6->a2a-samples==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from uvicorn>=0.34.0->a2a-samples==0.1.0) (8.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from anyio->httpx>=0.28.1->a2a-samples==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from cryptography>=3.4->jwcrypto>=1.5.6->a2a-samples==0.1.0) (1.17.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from opentelemetry-api>=1.33.0->a2a-sdk>=0.2.1->a2a-samples==0.1.0) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from opentelemetry-api>=1.33.0->a2a-sdk>=0.2.1->a2a-samples==0.1.0) (8.6.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from opentelemetry-sdk>=1.33.0->a2a-sdk>=0.2.1->a2a-samples==0.1.0) (0.54b1)\n",
      "Requirement already satisfied: pycparser in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from cffi>=1.14->cryptography>=3.4->jwcrypto>=1.5.6->a2a-samples==0.1.0) (2.22)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.33.0->a2a-sdk>=0.2.1->a2a-samples==0.1.0) (1.17.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.33.0->a2a-sdk>=0.2.1->a2a-samples==0.1.0) (3.21.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: llama_stack_client in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (0.2.7)\n",
      "Requirement already satisfied: asyncclick in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (8.1.8.0)\n",
      "Requirement already satisfied: nest_asyncio in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (1.6.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (8.1.7)\n",
      "Requirement already satisfied: dotenv in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (0.9.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (4.9.0)\n",
      "Requirement already satisfied: click in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (8.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (0.28.1)\n",
      "Requirement already satisfied: pandas in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (2.2.3)\n",
      "Requirement already satisfied: prompt-toolkit in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (3.0.51)\n",
      "Requirement already satisfied: pyaml in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (25.1.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (2.11.4)\n",
      "Requirement already satisfied: rich in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (14.0.0)\n",
      "Requirement already satisfied: sniffio in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (1.3.1)\n",
      "Requirement already satisfied: termcolor in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from llama_stack_client) (4.13.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipywidgets) (9.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: python-dotenv in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from dotenv) (1.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from anyio<5,>=3.5.0->llama_stack_client) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama_stack_client) (0.16.0)\n",
      "Requirement already satisfied: decorator in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from prompt-toolkit->llama_stack_client) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->llama_stack_client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->llama_stack_client) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->llama_stack_client) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pandas->llama_stack_client) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pandas->llama_stack_client) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pandas->llama_stack_client) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pandas->llama_stack_client) (2025.2)\n",
      "Requirement already satisfied: PyYAML in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pyaml->llama_stack_client) (6.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from rich->llama_stack_client) (3.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->llama_stack_client) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->llama_stack_client) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/kcogan/Documents/llama-stack-on-ocp/venv8/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install \"git+https://github.com/google/A2A.git#subdirectory=samples/python\"\n",
    "! pip install llama_stack_client asyncclick nest_asyncio ipywidgets dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49992c",
   "metadata": {},
   "source": [
    "Next, we'll add the necessary paths to `sys.path` to ensure we can import the required libraries later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49b3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# the path of the A2A library\n",
    "sys.path.append('./A2A/samples/python')\n",
    "# the path to our own utils\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1216b",
   "metadata": {},
   "source": [
    "We will now proceed with importing all the necessary Python libraries and modules for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a70138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.server import A2AServer\n",
    "from common.types import AgentCard, AgentSkill, AgentCapabilities\n",
    "from a2a_llama_stack.A2ATool import A2ATool\n",
    "from a2a_llama_stack.task_manager import AgentTaskManager\n",
    "\n",
    "# for asynchronously serving the A2A agent\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import urllib.parse\n",
    "from uuid import uuid4\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import subprocess\n",
    "import socket\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import threading\n",
    "import concurrent.futures as cf\n",
    "\n",
    "\n",
    "\n",
    "# Importing custom modules from the common package\n",
    "from common.client import A2AClient, A2ACardResolver\n",
    "from common.utils.push_notification_auth import PushNotificationReceiverAuth\n",
    "from hosts.cli.push_notification_listener import PushNotificationListener"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943937a",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5253d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: llama3.1:8b-instruct-fp16\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\", \"http://localhost:8321\")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\", \"llama3.1:8b-instruct-fp16\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb3a22f",
   "metadata": {},
   "source": [
    "We'll configure basic logging to provide informative output from the agents as they run, which can be very helpful for debugging and understanding the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86bde581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring basic logging for the application\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8fbb5",
   "metadata": {},
   "source": [
    "We need to use `nest_asyncio` to make sure things run smoothly when your Python code tries to do multiple tasks at the same time in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b7e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ac79e",
   "metadata": {},
   "source": [
    "## 2. Understanding the `AgentManager`\n",
    "\n",
    "The `AgentManager` class is key to connecting with and managing the different agents in our system.\n",
    "\n",
    "It handles:\n",
    "\n",
    "* Connecting to your orchestrator agent.\n",
    "\n",
    "* Connecting to individual skill agents (the ones that perform specific tasks).\n",
    "\n",
    "* Managing unique session IDs for each connection.\n",
    "\n",
    "* Using a helper function (`_send_payload`) to send tasks to the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7cea371",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentInfo = Tuple[str, Any, A2AClient, str]\n",
    "\n",
    "class AgentManager:\n",
    "    def __init__(self, urls: List[str]):\n",
    "        # first URL is your orchestrator…\n",
    "        self.orchestrator: AgentInfo = self._make_agent_info(urls[0])\n",
    "        # …the rest are skill agents, each keyed by skill.id\n",
    "        self.skills: Dict[str, AgentInfo] = {\n",
    "            skill.id: info\n",
    "            for url in urls[1:]\n",
    "            for info in (self._make_agent_info(url),)\n",
    "            for skill in info[1].skills\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_agent_info(url: str) -> AgentInfo:\n",
    "        card   = A2ACardResolver(url).get_agent_card()\n",
    "        client = A2AClient(agent_card=card)\n",
    "        session = uuid4().hex\n",
    "        return url, card, client, session\n",
    "\n",
    "\n",
    "async def _send_payload(client, card, session, payload, streaming: bool) -> str:\n",
    "    if not streaming:\n",
    "        res = await client.send_task(payload)\n",
    "        return res.result.status.message.parts[0].text.strip()\n",
    "\n",
    "    text = \"\"\n",
    "    async for ev in client.send_task_streaming(payload):\n",
    "        part = ev.result.status.message.parts[0].text or \"\"\n",
    "        print(part, end=\"\", flush=True)\n",
    "        text = part\n",
    "    print()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4d979",
   "metadata": {},
   "source": [
    "## 3. Preparing and Sending Tasks to Agents\n",
    "\n",
    "This section defines functions that format the user's question into a structured message (a JSON payload) that the agents can understand and process. It then uses the `_send_payload` helper from the `AgentManager` to send this task to the appropriate agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f9f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_skill_meta(mgr):\n",
    "    \"\"\"Gather metadata for every skill in all executor cards.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"skill_id\": s.id, \"name\": s.name,\n",
    "            \"description\": getattr(s, \"description\", None),\n",
    "            \"inputSchema\":  getattr(s, \"inputSchema\", None),\n",
    "            \"outputSchema\": getattr(s, \"outputSchema\", None),\n",
    "        }\n",
    "        for _, card, _, _ in mgr.skills.values()\n",
    "        for s in card.skills\n",
    "    ]\n",
    "\n",
    "async def _send_task_to_agent(mgr, client, card, session, question, push=False, host=None, port=None) -> str:\n",
    "    \"\"\"Build a card-driven payload (with optional push) and dispatch it.\"\"\"\n",
    "    # Skill metadata + input parts\n",
    "    skills = _build_skill_meta(mgr)\n",
    "    content = {\"skills\": skills, \"question\": question}\n",
    "    modes   = getattr(card, \"acceptedInputModes\", [\"text\"])\n",
    "    parts   = ([{\"type\": \"json\", \"json\": content}]\n",
    "               if \"json\" in modes\n",
    "               else [{\"type\": \"text\", \"text\": json.dumps(content)}])\n",
    "\n",
    "    # Optional push URL & auth\n",
    "    can_push = push and getattr(card.capabilities, \"pushNotifications\", False)\n",
    "    push_url = (urllib.parse.urljoin(f\"http://{host}:{port}\", \"/notify\")\n",
    "                if can_push and host and port else None)\n",
    "    schemes = getattr(card.authentication, \"supportedSchemes\", [\"bearer\"])\n",
    "\n",
    "    # Assemble payload\n",
    "    payload = {\n",
    "        \"id\": uuid4().hex,\n",
    "        \"sessionId\": session,\n",
    "        \"acceptedOutputModes\": card.defaultOutputModes,\n",
    "        \"message\": {\"role\": \"user\", \"parts\": parts},\n",
    "        **({\"pushNotification\": {\"url\": push_url,\n",
    "                                 \"authentication\": {\"schemes\": schemes}}}\n",
    "           if push_url else {})\n",
    "    }\n",
    "\n",
    "    # Dispatch, letting the card decide streaming vs one-shot\n",
    "    stream = getattr(card.capabilities, \"streaming\", False)\n",
    "    return await _send_payload(client, card, session, payload, stream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5862103",
   "metadata": {},
   "source": [
    "## 4. Defining the Agent Task Flow: Planning, Execution, and Composition\n",
    "This section contains the core logic for how the agents work together to answer a question. It's broken down into three distinct phases:\n",
    "\n",
    "* **Planning:** The orchestrator agent figures out the steps needed to answer the question and which skill agents to use.\n",
    "\n",
    "* **Execution:** The notebook code calls the necessary skill agents based on the plan and collects their results.\n",
    "\n",
    "* **Composition:** A final agent combines the results from the skill agents into a human-friendly answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86bd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _planning_phase(agent_manager, question, push, host, port):\n",
    "    \"\"\"Ask orchestrator for a plan, parse/fix JSON if necessary.\"\"\"\n",
    "    _, card, client, sess = agent_manager.orchestrator\n",
    "\n",
    "    raw = await _send_task_to_agent(agent_manager, client, card, sess, question, push=push, host=host, port=port)\n",
    "    print(f\"Raw plan ➡️ {raw}\")\n",
    "\n",
    "    try:\n",
    "        return json.loads(raw[: raw.rfind(\"]\") + 1])\n",
    "    except ValueError:\n",
    "        print(\"\\033[31mPlan parse failed, fixing invalid JSON...\\033[0m\")\n",
    "        fixer = \"Fix this json to be valid: \" + raw\n",
    "        fixed = await _send_task_to_agent(agent_manager, client, card, sess, fixer, push=push, host=host, port=port)\n",
    "        return json.loads(fixed)\n",
    "\n",
    "\n",
    "async def _execution_phase(agent_manager, plan, push, host, port):\n",
    "    \"\"\"Run each step in the plan via its skill and collect outputs.\"\"\"\n",
    "    results = []\n",
    "    for i, step in enumerate(plan, 1):\n",
    "        sid, inp = step[\"skill_id\"], json.dumps(step.get(\"input\", {}))\n",
    "        print(f\"➡️ Step {i}: {sid}({inp})\")\n",
    "\n",
    "        info = agent_manager.skills.get(sid)\n",
    "        if not info:\n",
    "            print(f\"\\033[31mNo executor for '{sid}', skipping.\\033[0m\")\n",
    "            results.append({\"skill_id\": sid, \"output\": None})\n",
    "            continue\n",
    "\n",
    "        _, skill_card, skill_client, skill_sess = info\n",
    "        out = await _send_task_to_agent(agent_manager, skill_client, skill_card, skill_sess, f\"{sid}({inp})\", push=push, host=host, port=port)\n",
    "        print(f\"   ✅ → {out}\")\n",
    "        results.append({\"skill_id\": sid, \"output\": out})\n",
    "\n",
    "    return results\n",
    "\n",
    "def _compose_prompt(parts, question):\n",
    "    \"\"\"Create the final composition prompt for the orchestrator.\"\"\"\n",
    "    return (\n",
    "        f\"Using the following information: {json.dumps(parts)}, \"\n",
    "        f\"write a clear and human-friendly response to the question: '{question}'. \"\n",
    "        \"Keep it concise and easy to understand and respond like a human with character. \"\n",
    "        \"Only use the information provided. If you cannot answer the question, say 'I don't know'. \"\n",
    "        \"Never show any code or JSON, just the answer.\\n\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d3fe2",
   "metadata": {},
   "source": [
    "## 5. Orchestrating the Agent Interaction\n",
    "This is the main function that ties together the planning, execution, and composition phases to answer a user's question using the agent team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d7612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_question(\n",
    "    agent_manager: AgentManager,\n",
    "    question: str,\n",
    "    push: bool = False,\n",
    "    push_receiver: str = \"http://localhost:5000\",\n",
    ") -> str:\n",
    "    # Unpack orchestrator info\n",
    "    orch_url, orch_card, orch_client, orch_session = agent_manager.orchestrator\n",
    "\n",
    "    # Optionally start push listener\n",
    "    host = port = None\n",
    "    if push:\n",
    "        parsed = urllib.parse.urlparse(push_receiver)\n",
    "        host, port = parsed.hostname, parsed.port\n",
    "        auth = PushNotificationReceiverAuth()\n",
    "        await auth.load_jwks(f\"{orch_url}/.well-known/jwks.json\")\n",
    "        PushNotificationListener(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            notification_receiver_auth=auth\n",
    "        ).start()\n",
    "\n",
    "    # --- Planning Phase ---\n",
    "    print(\"\\n\\033[1;33m=========== 🧠 Planning Phase ===========\\033[0m\")\n",
    "    plan = await _planning_phase(agent_manager, question, push=push, host=host, port=port)\n",
    "    print(f\"\\n\\033[1;32mFinal plan ➡️ {plan}\\033[0m\")\n",
    "\n",
    "    # --- Execution Phase ---\n",
    "    print(\"\\n\\033[1;33m=========== ⚡️ Execution Phase ===========\\033[0m\")\n",
    "    parts = await _execution_phase(agent_manager, plan, push=push, host=host, port=port)\n",
    "\n",
    "    # --- Composing Answer ---\n",
    "    print(\"\\n\\033[1;33m=========== 🛠️ Composing Answer ===========\\033[0m\")\n",
    "    comp_prompt = _compose_prompt(parts, question)\n",
    "    WRITING_AGENT_ID   = \"writing_agent\"\n",
    "\n",
    "    _, skill_card, skill_client, skill_sess = agent_manager.skills.get(WRITING_AGENT_ID)\n",
    "    final = await _send_task_to_agent(agent_manager, skill_client, skill_card, skill_sess, comp_prompt, push=push, host=host, port=port)\n",
    "\n",
    "    print(\"\\n\\033[1;36m🎉 FINAL ANSWER\\033[0m\")\n",
    "    print(final)\n",
    "    print(\"\\033[1;36m====================================\\033[0m\")\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d580d4f",
   "metadata": {},
   "source": [
    "## 6. Launching the A2A Agent Servers\n",
    "Before we can interact with the agents, we need to start their servers. This bootstrap script handles bringing the complete A2A stack online.\n",
    "\n",
    "It performs three key actions in sequence:\n",
    "\n",
    "1. Defines connection details (ports and modules).\n",
    "\n",
    "2. Starts each agent in parallel and waits for them to be ready.\n",
    "\n",
    "3. Connects to the running agents and summarizes their status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd2d20",
   "metadata": {},
   "source": [
    "Below, we set up the network addresses (`URLS`) for our orchestrator and skill agents, and specify the Python modules that implement their functionality. These definitions are crucial for starting and connecting to the agents in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceb43397",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORCHESTRATOR_URL = \"http://localhost:10010\"\n",
    "EXECUTOR_URLS    = [\"http://localhost:10011\", \"http://localhost:10012\"]\n",
    "URLS             = [ORCHESTRATOR_URL, *EXECUTOR_URLS]\n",
    "MODULES          = [\n",
    "    \"agents.a2a_planner\",\n",
    "    \"agents.a2a_custom_tools\",\n",
    "    \"agents.a2a_composer\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9580373e",
   "metadata": {},
   "source": [
    "This launches the agent processes and wait until each server is ready and listening on its assigned port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3083dd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ agents.a2a_planner ready on port 10010\n",
      "✅ agents.a2a_custom_tools ready on port 10011\n",
      "✅ agents.a2a_composer ready on port 10012\n"
     ]
    }
   ],
   "source": [
    "os.chdir('..') # change to the directory where the script is located\n",
    "\n",
    "def _launch(mod, url):\n",
    "    port = int(url.split(\":\")[-1])\n",
    "    subprocess.Popen([sys.executable, \"-m\", mod, \"--port\", str(port)],\n",
    "                     stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    while socket.socket().connect_ex((\"127.0.0.1\", port)): time.sleep(.1)\n",
    "    return f\"✅ {mod} ready on port {port}\"\n",
    "\n",
    "with cf.ThreadPoolExecutor() as pool:\n",
    "    print(*pool.map(_launch, MODULES, URLS), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c1a74a",
   "metadata": {},
   "source": [
    "Now that the agents should be running, we'll use our `AgentManager` to connect to them, confirm they are online, and see what skills they offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ea51ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:02:19,356 INFO httpx: HTTP Request: GET http://localhost:10010/.well-known/agent.json \"HTTP/1.1 200 OK\"\n",
      "2025-05-20 16:02:19,366 INFO httpx: HTTP Request: GET http://localhost:10011/.well-known/agent.json \"HTTP/1.1 200 OK\"\n",
      "2025-05-20 16:02:19,376 INFO httpx: HTTP Request: GET http://localhost:10012/.well-known/agent.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m===================== 🛰️ Connected Agents =====================\u001b[0m\n",
      "Orchestrator: http://localhost:10010 (Orchestration Agent)\n",
      "Executors:\n",
      "  • random_number_tool -> http://localhost:10011 (Custom Agent)\n",
      "  • date_tool -> http://localhost:10011 (Custom Agent)\n",
      "  • writing_agent -> http://localhost:10012 (Writing Agent)\n",
      "\u001b[1;36m===============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_agent_manager = AgentManager(URLS)\n",
    "orch_url, orch_card, *_ = _agent_manager.orchestrator\n",
    "\n",
    "print(\"\\n\\033[1;36m===================== 🛰️ Connected Agents =====================\\033[0m\")\n",
    "print(f\"Orchestrator: {orch_url} ({orch_card.name})\")\n",
    "print(\"Executors:\")\n",
    "for sid, (u, card, *_) in _agent_manager.skills.items():\n",
    "    print(f\"  • {sid} -> {u} ({card.name})\")\n",
    "print(\"\\033[1;36m===============================================================\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32892ab",
   "metadata": {},
   "source": [
    "## 7. Asking the Agent Team a Question!\n",
    "\n",
    "Finally, it's time to put our agent team to work! We'll use the `ask_question` function we defined earlier to send our queries and see the multi-agent system in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6476a816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;33m=========== 🧠 Planning Phase ===========\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:02:31,947 INFO httpx: HTTP Request: POST http://0.0.0.0:10010/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw plan ➡️ [\n",
      "  {\"skill_id\": \"date_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"},\n",
      "  {\"skill_id\": \"random_number_tool\"}\n",
      "]\n",
      "\n",
      "\u001b[1;32mFinal plan ➡️ [{'skill_id': 'date_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}, {'skill_id': 'random_number_tool'}]\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== ⚡️ Execution Phase ===========\u001b[0m\n",
      "➡️ Step 1: date_tool({})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:02:37,550 INFO httpx: HTTP Request: POST http://0.0.0.0:10011/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ → {\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": {}}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-05-20\"The date today is 2023-05-20.\n",
      "➡️ Step 2: random_number_tool({})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:02:41,982 INFO httpx: HTTP Request: POST http://0.0.0.0:10011/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ → {\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:1The random number generated is 1.\n",
      "➡️ Step 3: random_number_tool({})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:02:46,690 INFO httpx: HTTP Request: POST http://0.0.0.0:10011/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ → {\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:86The random number generated is 86.\n",
      "➡️ Step 4: random_number_tool({})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:02:51,154 INFO httpx: HTTP Request: POST http://0.0.0.0:10011/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ → {\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:20The random number generated is 20.\n",
      "➡️ Step 5: random_number_tool({})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:02:55,637 INFO httpx: HTTP Request: POST http://0.0.0.0:10011/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ → {\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:5The random number generated is 5.\n",
      "➡️ Step 6: random_number_tool({})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:00,145 INFO httpx: HTTP Request: POST http://0.0.0.0:10011/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ → {\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:44The random number generated is 44.\n",
      "\n",
      "\u001b[1;33m=========== 🛠️ Composing Answer ===========\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:05,302 INFO httpx: HTTP Request: POST http://0.0.0.0:10012/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m🎉 FINAL ANSWER\u001b[0m\n",
      "Here's your response:\n",
      "\n",
      "\"Today's date is May 20th, 2023. Here are five random numbers: 1, 86, 20, 5, and 44.\"\n",
      "\u001b[1;36m====================================\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== 🧠 Planning Phase ===========\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:11,257 INFO httpx: HTTP Request: POST http://0.0.0.0:10010/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw plan ➡️ [\n",
      "  {\"skill_id\": \"date_tool\"}\n",
      "]\n",
      "\n",
      "\u001b[1;32mFinal plan ➡️ [{'skill_id': 'date_tool'}]\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== ⚡️ Execution Phase ===========\u001b[0m\n",
      "➡️ Step 1: date_tool({})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:28,192 INFO httpx: HTTP Request: POST http://0.0.0.0:10011/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ → {\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": {}}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-05-20\"The current date is May 20, 2025.\n",
      "\n",
      "\u001b[1;33m=========== 🛠️ Composing Answer ===========\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:30,939 INFO httpx: HTTP Request: POST http://0.0.0.0:10012/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m🎉 FINAL ANSWER\u001b[0m\n",
      "\"Today's date is May 20, 2025.\"\n",
      "\u001b[1;36m====================================\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== 🧠 Planning Phase ===========\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:37,648 INFO httpx: HTTP Request: POST http://0.0.0.0:10010/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw plan ➡️ [\n",
      "  {\"skill_id\": \"random_number_tool\"}\n",
      "]\n",
      "\n",
      "\u001b[1;32mFinal plan ➡️ [{'skill_id': 'random_number_tool'}]\u001b[0m\n",
      "\n",
      "\u001b[1;33m=========== ⚡️ Execution Phase ===========\u001b[0m\n",
      "➡️ Step 1: random_number_tool({})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:54,412 INFO httpx: HTTP Request: POST http://0.0.0.0:10011/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ → {\"type\": \"function\", \"name\": \"random_number_tool\", \"parameters\": {}}Tool:random_number_tool Args:{}Tool:random_number_tool Response:21The random number generated is 21.\n",
      "\n",
      "\u001b[1;33m=========== 🛠️ Composing Answer ===========\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:57,080 INFO httpx: HTTP Request: POST http://0.0.0.0:10012/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;36m🎉 FINAL ANSWER\u001b[0m\n",
      "\"Here's a random number: 21.\"\n",
      "\u001b[1;36m====================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "questions = [ \n",
    "    \"Get todays date then generate five random numbers\",\n",
    "    \"Get todays date?\",\n",
    "    \"generate a random number\",\n",
    "    ]\n",
    "\n",
    "for question in questions:\n",
    "    await ask_question(_agent_manager, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c00af",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "Congratulations! You've successfully set up and interacted with a multi-agent system using the A2A protocol. You saw how an orchestrator agent planned the task, how skill agents executed the steps, and how a composition agent put it all together for you.\n",
    "\n",
    "Future demos will cover more advanced aspects of agent-to-agent communication."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
