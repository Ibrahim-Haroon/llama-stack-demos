{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef1ba28",
   "metadata": {},
   "source": [
    "# A simple agent A2A Application with Custom Tools\n",
    "\n",
    "This notebook presents a simple scenario where an agent uses the A2A protocol to query another agent for information using custom tools. We show how to initialize an agent in Llama Stack and grant it access to communicating with another, external agent.\n",
    "\n",
    "This demo demonstrates core A2A communication principles.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Setting up a Llama Stack agent with custom tool capabilities (e.g., random number generation, date retrieval).\n",
    "2. Serving this agent over an A2A server.\n",
    "3. Initializing another Llama Stack agent capable of communicating with the custom tool agent.\n",
    "4. Launching the second agent and using it to answer user queries by leveraging the first agent's tools.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "- `python_requires >= 3.13`\n",
    "\n",
    "- Followed the instructions in the [Setup Guide](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb) notebook.\n",
    "\n",
    "## Additional environment variables\n",
    "This demo requires the following environment variables in addition to those defined in the [Setup Guide](../../rag_agentic/notebooks//Level0_getting_started_with_Llama_Stack.ipynb):\n",
    "- `CUSTOM_TOOL_AGENT_LOCAL_PORT`: the port over which we will serve the exported A2A agent with custom tool capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c88e09",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "To provide A2A communication capabilities, we will use the [sample implementation by Google](https://github.com/google/A2A/tree/main/samples/python). Please make sure that the content of the referenced directory is available on your Python path. This can be done, for example, by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c195db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'a2a-samples' already exists and is not an empty directory.\n",
      "Requirement already satisfied: annotated-types==0.7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: anyio==4.9.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: appnope==0.1.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 3)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: certifi==2025.1.31 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 5)) (2025.1.31)\n",
      "Requirement already satisfied: cffi==1.17.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 6)) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer==3.4.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: click==8.1.8 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 8)) (8.1.8)\n",
      "Requirement already satisfied: comm==0.2.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 9)) (0.2.2)\n",
      "Requirement already satisfied: cryptography==45.0.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 10)) (45.0.3)\n",
      "Requirement already satisfied: debugpy==1.8.14 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 11)) (1.8.14)\n",
      "Requirement already satisfied: decorator==5.2.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 12)) (5.2.1)\n",
      "Requirement already satisfied: distro==1.9.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 13)) (1.9.0)\n",
      "Requirement already satisfied: dotenv==0.9.9 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 14)) (0.9.9)\n",
      "Requirement already satisfied: executing==2.2.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 15)) (2.2.0)\n",
      "Requirement already satisfied: fire==0.7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 16)) (0.7.0)\n",
      "Requirement already satisfied: h11==0.16.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 17)) (0.16.0)\n",
      "Requirement already satisfied: httpcore==1.0.9 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 18)) (1.0.9)\n",
      "Requirement already satisfied: httpx==0.28.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 19)) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 20)) (0.4.0)\n",
      "Requirement already satisfied: idna==3.10 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 21)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 22)) (6.29.5)\n",
      "Requirement already satisfied: ipython==9.3.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 23)) (9.3.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 24)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 25)) (0.19.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 26)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.8.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 27)) (5.8.1)\n",
      "Requirement already satisfied: jwcrypto==1.5.6 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 28)) (1.5.6)\n",
      "Requirement already satisfied: llama_stack_client==0.2.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 29)) (0.2.2)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 30)) (3.0.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 31)) (0.1.7)\n",
      "Requirement already satisfied: mdurl==0.1.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 32)) (0.1.2)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 33)) (1.6.0)\n",
      "Requirement already satisfied: numpy==2.2.5 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 34)) (2.2.5)\n",
      "Requirement already satisfied: packaging==25.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 35)) (25.0)\n",
      "Requirement already satisfied: pandas==2.2.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 36)) (2.2.3)\n",
      "Requirement already satisfied: parso==0.8.4 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 37)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 38)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.3.8 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 39)) (4.3.8)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 40)) (3.0.51)\n",
      "Requirement already satisfied: psutil==7.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 41)) (7.0.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 42)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 43)) (0.2.3)\n",
      "Requirement already satisfied: pyaml==25.1.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 44)) (25.1.0)\n",
      "Requirement already satisfied: pycparser==2.22 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 45)) (2.22)\n",
      "Requirement already satisfied: pydantic==2.11.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 46)) (2.11.3)\n",
      "Requirement already satisfied: pydantic_core==2.33.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 47)) (2.33.1)\n",
      "Requirement already satisfied: Pygments==2.19.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 48)) (2.19.1)\n",
      "Requirement already satisfied: PyJWT==2.10.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 49)) (2.10.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 50)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv==1.1.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 51)) (1.1.0)\n",
      "Requirement already satisfied: pytz==2025.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 52)) (2025.2)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 53)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 54)) (26.4.0)\n",
      "Requirement already satisfied: requests==2.32.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 55)) (2.32.3)\n",
      "Requirement already satisfied: rich==14.0.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 56)) (14.0.0)\n",
      "Requirement already satisfied: six==1.17.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 57)) (1.17.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 58)) (1.3.1)\n",
      "Requirement already satisfied: sse-starlette==2.2.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 59)) (2.2.1)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 60)) (0.6.3)\n",
      "Requirement already satisfied: starlette==0.46.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 61)) (0.46.2)\n",
      "Requirement already satisfied: termcolor==3.0.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 62)) (3.0.1)\n",
      "Requirement already satisfied: tornado==6.5.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 63)) (6.5.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 64)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 65)) (5.14.3)\n",
      "Requirement already satisfied: typing-inspection==0.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 66)) (0.4.0)\n",
      "Requirement already satisfied: typing_extensions==4.13.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 67)) (4.13.2)\n",
      "Requirement already satisfied: tzdata==2025.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 68)) (2025.2)\n",
      "Requirement already satisfied: urllib3==2.4.0 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 69)) (2.4.0)\n",
      "Requirement already satisfied: uvicorn==0.34.2 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 70)) (0.34.2)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /Users/kcogan/Documents/llama-stack-on-ocp/venv5/lib/python3.13/site-packages (from -r ../requirements.txt (line 71)) (0.2.13)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/google-a2a/a2a-samples.git\n",
    "! pip install -r \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192c7e7",
   "metadata": {},
   "source": [
    "Now, we will add the paths to the A2A library and our own tools to `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a99e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# the path of the A2A library\n",
    "sys.path.append('./a2a-samples/samples/python')\n",
    "# the path to our own utils\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be7200",
   "metadata": {},
   "source": [
    "We will now proceed with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26570e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.server import A2AServer\n",
    "from common.types import AgentCard, AgentSkill, AgentCapabilities\n",
    "from a2a_llama_stack.A2ATool import A2ATool\n",
    "from a2a_llama_stack.task_manager import AgentTaskManager\n",
    "\n",
    "# for asynchronously serving the A2A agent\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f018c",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](../../rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de3ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: llama3.1:8b-instruct-fp16\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# agent related imports\n",
    "import uuid\n",
    "from llama_stack_client import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "# pretty print of the results returned from the model/agent - import from the rag_agentic demo subdirectory\n",
    "import sys\n",
    "sys.path.append('../../rag_agentic')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6631ae7",
   "metadata": {},
   "source": [
    "## 2. Setting Up and Serving a Simple A2A Agent with Custom Tools\n",
    "We will now initialize an agent with custom tools (random number generator and date tool) and make it available via an A2A server.\n",
    "\n",
    "Our first steps involve defining these custom tools and then creating an agent that knows how to use them.\n",
    "- Define Python functions that will serve as our tools.\n",
    "- Initialize a Llama Stack agent, providing it with these tools and instructions on how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972b1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def random_number_tool() -> int:\n",
    "    \"\"\"\n",
    "    Generate a random integer between 1 and 100.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\nGenerating a random number...\\n\\n\")\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "\n",
    "def date_tool() -> str:\n",
    "    \"\"\"\n",
    "    Return today's date in YYYY-MM-DD format.\n",
    "    \"\"\"\n",
    "    return datetime.utcnow().date().isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934c92d",
   "metadata": {},
   "source": [
    "- Initialize a Llama Stack agent with a list of tools including the built-in RAG tool. The RAG tool specification must include a list of document collection IDs to retrieve from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b71d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tool_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=(\n",
    "            \"You have access to two tools:\\n\"\n",
    "            \"- random_number_tool: generates one random integer between 1 and 100\\n\"\n",
    "            \"- date_tool: returns today's date in YYYY-MM-DD format\\n\"\n",
    "            \"Always use the appropriate tool to answer user queries.\"\n",
    "        ),    \n",
    "    sampling_params=sampling_params,\n",
    "    tools=[random_number_tool, date_tool],\n",
    "    max_infer_iters=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73064c0",
   "metadata": {},
   "source": [
    "Now, our Llama Stack agent is ready to be served as an A2A agent. This includes the following steps:\n",
    " - Create an `AgentCard` - an object containing all the details about the agent we are about to serve, including its URL and exposed capabilities.\n",
    " - Wrap the Llama Stack agent with an `AgentTaskManager` object - a wrapper/adapter making it possible for the A2A server to redirect incoming request to the Llama Stack agent.\n",
    " - Create and launch an `A2AServer` - a Rest API server capable of communicating via the A2A protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab56668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [4581]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:10020 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:49654 - \"GET /.well-known/agent.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/635191ns4599kwjkqt12kwd80000gn/T/ipykernel_4581/2943416845.py:17: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().date().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:49667 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:49696 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "custom_tool_agent_local_port = int(os.getenv(\"CUSTOM_TOOL_AGENT_LOCAL_PORT\", \"10020\"))\n",
    "custom_tool_agent_url = f\"http://localhost:{custom_tool_agent_local_port}\"\n",
    "\n",
    "agent_card = AgentCard(\n",
    "    name=\"Custom Agent\",\n",
    "    description=\"Generates random numbers or retrieve today's dates\",\n",
    "    url=custom_tool_agent_url,\n",
    "    version=\"0.1.0\",\n",
    "    defaultInputModes=[\"text/plain\"],\n",
    "    defaultOutputModes=[\"text/plain\"],\n",
    "    capabilities=AgentCapabilities(\n",
    "        streaming=True,\n",
    "        pushNotifications=False,\n",
    "        stateTransitionHistory=False,\n",
    "        ),\n",
    "    skills=[\n",
    "        AgentSkill(\n",
    "            id=\"random_number_tool\", \n",
    "            name=\"Random Number Generator\",\n",
    "            description=\"Generates a random number between 1 and 100\",\n",
    "            tags=[\"random\"],\n",
    "            examples=[\"Give me a random number between 1 and 100\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"text/plain\"],\n",
    "            ),\n",
    "        AgentSkill(\n",
    "            id=\"date_tool\",\n",
    "            name=\"Date Provider\",\n",
    "            description=\"Returns today's date in YYYY-MM-DD format\",\n",
    "            tags=[\"date\"],\n",
    "            examples=[\"What's the date today?\"],\n",
    "            inputModes=[\"text/plain\"],\n",
    "            outputModes=[\"text/plain\"],\n",
    "            ),\n",
    "    ],\n",
    ")\n",
    "task_manager = AgentTaskManager(agent=custom_tool_agent)\n",
    "server = A2AServer(\n",
    "    agent_card=agent_card,\n",
    "    task_manager=task_manager,\n",
    "    host='localhost',\n",
    "    port=custom_tool_agent_local_port\n",
    ")\n",
    "thread = threading.Thread(target=server.start, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dbee6",
   "metadata": {},
   "source": [
    "## 3. Setting up an agent capable of A2A communication with the CUSTOM_TOOL agent\n",
    "This includes the following steps:\n",
    " - Create a Llama Stack client tool that wraps A2A communication with the CUSTOM_TOOL agent.\n",
    " - Initialize a client agent with access to the above client tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ece2521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tool_agent_tool = A2ATool(custom_tool_agent_url)\n",
    "a2a_client_agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=\"You are a helpful assistant. When a tool is used, only print its output without adding more content.\",\n",
    "    sampling_params=sampling_params,\n",
    "    tools=[custom_tool_agent_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf46ddf5",
   "metadata": {},
   "source": [
    "Now, let's use our client agent for serving user requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da019ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "User> What is today's date?\u001b[0m\n",
      "\n",
      "---------- 📍 Step 1: InferenceStep ----------\n",
      "🛠️ Tool call Generated:\n",
      "\u001b[35mTool call: Custom Agent, Arguments: {'query': \"today's date\"}\u001b[0m\n",
      "\n",
      "---------- 📍 Step 2: ToolExecutionStep ----------\n",
      "🔧 Executing tool...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'{\\n    \"type\": \"function\",\\n    \"name\": \"date_tool\",\\n    \"parameters\": {}\\n}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-03\"{\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": {}}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-03\"I used the date_tool function to get today\\'s date.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    \"type\": \"function\",\\n    \"name\": \"date_tool\",\\n    \"parameters\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Args:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Response:\"2025-06-03\"\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Args:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Response:\"2025-06-03\"I used the date_tool function to get today\\'s date.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- 📍 Step 3: InferenceStep ----------\n",
      "🛠️ Tool call Generated:\n",
      "\u001b[35mTool call: Custom Agent, Arguments: {'query': \"today's date\"}\u001b[0m\n",
      "\n",
      "---------- 📍 Step 4: ToolExecutionStep ----------\n",
      "🔧 Executing tool...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'{\\n    \"type\": \"function\",\\n    \"name\": \"date_tool\",\\n    \"parameters\": {}\\n}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-03\"{\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": {}}Tool:date_tool Args:{}Tool:date_tool Response:\"2025-06-03\"I used the date_tool function to get today\\'s date.'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n    \"type\": \"function\",\\n    \"name\": \"date_tool\",\\n    \"parameters\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Args:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Response:\"2025-06-03\"\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"type\": \"function\", \"name\": \"date_tool\", \"parameters\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Args:\u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32mTool:date_tool Response:\"2025-06-03\"I used the date_tool function to get today\\'s date.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- 📍 Step 5: InferenceStep ----------\n",
      "🤖 Model Response:\n",
      "\u001b[35mThe current date is 2025-06-03.\n",
      "\u001b[0m\n",
      "========== Query processing completed ========== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is today's date?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # create a new turn with a new session ID for each prompt\n",
    "    response = a2a_client_agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=a2a_client_agent.create_session(f\"agent-session_{uuid.uuid4()}\"),\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    # print the response, including tool calls output\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
